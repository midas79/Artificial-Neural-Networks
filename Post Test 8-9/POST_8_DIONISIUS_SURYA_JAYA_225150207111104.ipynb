{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvonXDMsQP1k"
      },
      "source": [
        "# Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YzH-ilE2twTF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gGbt71BdLJV"
      },
      "source": [
        "Pada post-test kali ini akan membandingkan dua jenis fungsi aktivasi yang biasa digunakan dalam backpropogation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "BTLa3NWvz7sq"
      },
      "outputs": [],
      "source": [
        "#Fungsi Aktivasi Sigmoid dengan turunannya\n",
        "def sig(X):\n",
        "  return [1 / (1 + np.exp(-x)) for x in X]\n",
        "\n",
        "def sigd(X):\n",
        "  output = []\n",
        "\n",
        "  for i, x in enumerate(X):\n",
        "      s = sig([x])[0]\n",
        "      output.append(s * (1 - s))\n",
        "  return output\n",
        "\n",
        "#Fungsi Aktivasi Hyperbolic Tangent dengan turunannya\n",
        "def tanh(X):\n",
        "  return [np.tanh(x) for x in X]\n",
        "\n",
        "def tanhd(X):\n",
        "  output = []\n",
        "\n",
        "  for x in X:\n",
        "      t = np.tanh(x)\n",
        "      output.append(1 - t ** 2)\n",
        "  return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "MopOydIkUjtH"
      },
      "outputs": [],
      "source": [
        "def onehot_enc(lbl, min_val=0):\n",
        "  mi = min(lbl)\n",
        "  enc = np.full((len(lbl), max(lbl) - mi + 1), min_val, np.int8)\n",
        "\n",
        "  for i, x in enumerate(lbl):\n",
        "    enc[i, x - mi] = 1\n",
        "\n",
        "  return enc\n",
        "\n",
        "def onehot_dec(enc, mi=0):\n",
        "  return [np.argmax(e) + mi for e in enc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hihqFCY_ctZ3"
      },
      "source": [
        "### a) Fungsi *Training* Backpropagation\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "pTlk5igwcvc5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def bp_fit_sig(X, target, layer_conf, max_epoch, max_error=0.1, learn_rate=0.1, print_per_epoch=100):\n",
        "    start_time = time.time()\n",
        "    np.random.seed(1)\n",
        "\n",
        "    n = [np.empty(j + 1) if i < len(layer_conf) - 1 else np.empty(j) for i, j in enumerate(layer_conf)]\n",
        "    w = [np.random.rand(layer_conf[i] + 1, layer_conf[i + 1]) for i in range(len(layer_conf) - 1)]\n",
        "    dw = [np.zeros((layer_conf[i] + 1, layer_conf[i + 1])) for i in range(len(layer_conf) - 1)]\n",
        "    d = [np.zeros(s) for s in layer_conf[1:]]\n",
        "\n",
        "    epoch = 0\n",
        "    mse = 1\n",
        "\n",
        "    for i in range(0, len(n) - 1):\n",
        "        n[i][-1] = 1\n",
        "\n",
        "    while (max_epoch == -1 or epoch < max_epoch) and mse > max_error:\n",
        "        epoch += 1\n",
        "        mse = 0\n",
        "\n",
        "        for r in range(len(X)):\n",
        "            n[0][:-1] = X[r]\n",
        "            for L in range(1, len(layer_conf)):\n",
        "                nin = np.dot(n[L - 1], w[L - 1])\n",
        "                n[L][:len(nin)] = sig(nin)\n",
        "\n",
        "            e = target[r] - n[-1]\n",
        "            mse += np.sum(e ** 2)\n",
        "\n",
        "            d[-1] = e * sigd(n[-1])\n",
        "            dw[-1] = learn_rate * np.outer(n[-2], d[-1])\n",
        "\n",
        "            for L in range(len(layer_conf) - 2, 0, -1):\n",
        "                din = np.dot(d[L], w[L][:-1].T)\n",
        "                d[L - 1] = din * sigd(n[L][:-1])\n",
        "                dw[L - 1] = learn_rate * np.outer(n[L - 1], d[L - 1])\n",
        "\n",
        "            for i in range(len(w)):\n",
        "                w[i] += dw[i]\n",
        "\n",
        "        mse /= len(X)\n",
        "\n",
        "        if print_per_epoch > -1 and epoch % print_per_epoch == 0:\n",
        "            print(f'Epoch {epoch}, MSE: {mse}')\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "    print(\"Waktu eksekusi: %s detik\" % execution_time)\n",
        "\n",
        "    return w, epoch, mse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Pet6ptVOTxUR"
      },
      "outputs": [],
      "source": [
        "def bp_fit_tanh(X, target, layer_conf, max_epoch, max_error=.1, learn_rate=.1, print_per_epoch=100):\n",
        "    start_time = time.time()\n",
        "    np.random.seed(1)\n",
        "    nin = [np.empty(i) for i in layer_conf]\n",
        "    n = [np.empty(j + 1) if i < len(layer_conf) - 1 else np.empty(j) for i, j in enumerate(layer_conf)]\n",
        "    w = [np.random.rand(layer_conf[i] + 1, layer_conf[i + 1]) for i in range(len(layer_conf) - 1)]\n",
        "    dw = [np.empty((layer_conf[i] + 1, layer_conf[i + 1])) for i in range(len(layer_conf) - 1)]\n",
        "    d = [np.empty(s) for s in layer_conf[1:]]\n",
        "    din = [np.empty(s) for s in layer_conf[1:-1]]\n",
        "    epoch = 0\n",
        "    mse = 1\n",
        "    for i in range(0, len(n)-1):\n",
        "        n[i][-1] = 1\n",
        "    while (max_epoch == -1 or epoch < max_epoch) and mse > max_error:\n",
        "        epoch += 1\n",
        "        mse = 0\n",
        "        for r in range(len(X)):\n",
        "            n[0][:-1] = X[r]\n",
        "            for L in range(1, len(layer_conf)):\n",
        "                nin[L] = np.dot(n[L-1], w[L-1])\n",
        "                n[L][:len(nin[L])] = tanh(nin[L])\n",
        "            e = target[r] - n[-1]\n",
        "            mse += sum(e ** 2)\n",
        "            d[-1] = e * tanhd(nin[-1])\n",
        "            dw[-1] = learn_rate * d[-1] * n[-2].reshape((-1, 1))\n",
        "            for L in range(len(layer_conf) - 1, 1, -1):\n",
        "                din[L-2] = np.dot(d[L-1], np.transpose(w[L-1][:-1]))\n",
        "                d[L-2] = din[L-2] * np.array(tanhd(nin[L-1]))\n",
        "                dw[L-2] = (learn_rate * d[L-2]) * n[L-2].reshape((-1, 1))\n",
        "            w[L-1] += dw[L-1]\n",
        "        mse /= len(X)\n",
        "        if print_per_epoch > -1 and epoch % print_per_epoch == 0:\n",
        "            print(f'Epoch {epoch}, MSE: {mse}')\n",
        "    execution = time.time() - start_time\n",
        "    print(\"Waktu eksekusi: %s detik\" % execution)\n",
        "    return w, epoch, mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJA_9btdc3ED"
      },
      "source": [
        "### b) Fungsi *Testing* Backpropagation\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "2zyXIu_ec9go"
      },
      "outputs": [],
      "source": [
        "def bp_predict_sig(X, w):\n",
        "  n = [np.empty(len(i)) for i in w]\n",
        "  nin = [np.empty(len(i[0])) for i in w]\n",
        "  predict = []\n",
        "  n.append(np.empty(len(w[-1][0])))\n",
        "  for x in X:\n",
        "    n[0][:-1] = x\n",
        "    for L in range(0, len(w)):\n",
        "      nin[L] = np.dot(n[L], w[L])\n",
        "      n[L + 1][:len(nin[L])] = sig(nin[L])\n",
        "    predict.append(n[-1].copy())\n",
        "  return predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "paHySilia3gw"
      },
      "outputs": [],
      "source": [
        "#Membuat fungsi testing backpropagation dengan menggunakan fungsi aktivasi tanh\n",
        "def bp_predict_tanh(X, w):\n",
        "    n = [np.empty(len(i)) for i in w]\n",
        "    nin = [np.empty(len(i[0])) for i in w]\n",
        "    predict = []\n",
        "    n.append(np.empty(len(w[-1][0])))\n",
        "    for x in X:\n",
        "        n[0][:-1] = x\n",
        "        for L in range(0, len(w)):\n",
        "            nin[L] = np.dot(n[L], w[L])\n",
        "            n[L + 1][:len(nin[L])] = tanh(nin[L])\n",
        "        predict.append(n[-1].copy())\n",
        "    return predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZxy_M5Jc-ko"
      },
      "source": [
        "### c) Klasifikasi dataset wine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xj7DqCdudcF"
      },
      "source": [
        "Lakukan pelatihan pada dataset wine dengan menggunakan 2 fungsi pelatihan yang telah dibuat!\n",
        "\n",
        "Konfigurasi kedua pelatihan harus sama (epoch, hidden layer, learning rate, dll).\n",
        "Akurasi yang diharapkan di setiap pelatihan adalah > 0.98"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "us5Kwtn5trf0"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw1L_Q3JdHk7",
        "outputId": "92100d6f-884e-48b7-e09c-08766a394f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25, MSE: 0.4425340061469445\n",
            "Epoch 50, MSE: 0.3322178999354772\n",
            "Epoch 75, MSE: 0.13894317248490345\n",
            "Epoch 100, MSE: 0.14104871228706387\n",
            "Waktu eksekusi: 2.1015806198120117 detik\n",
            "Epochs: 100, MSE: 0.14104871228706387\n",
            "Output: [2, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0, 1, 2, 0, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0]\n",
            "True : [2, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0]\n",
            "Accuracy: 0.9444444444444444\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X = minmax_scale(wine.data)\n",
        "Y = onehot_enc(wine.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.3,random_state=1)\n",
        "#Isi jumlah layer yang digunakan dengan jumlah hidden layer 8\n",
        "w, ep, mse = bp_fit_sig(X_train, y_train, layer_conf=(13, 8, 3),learn_rate=0.1, max_epoch=100, max_error=0.1, print_per_epoch=25)\n",
        "\n",
        "print(f'Epochs: {ep}, MSE: {mse}')\n",
        "\n",
        "predict = bp_predict_sig(X_test, w)\n",
        "predict = onehot_dec(predict)\n",
        "y_test = onehot_dec(y_test)\n",
        "accuracy = accuracy_score(predict, y_test)\n",
        "\n",
        "print('Output:', predict)\n",
        "print('True :', y_test)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF18HdQgbCXy",
        "outputId": "919f1001-bb35-4344-b73a-7c28998dffcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1000, MSE: 0.6849287877208196\n",
            "Epoch 2000, MSE: 0.6056967502504933\n",
            "Epoch 3000, MSE: 0.565134862766249\n",
            "Epoch 4000, MSE: 0.5418064255715432\n",
            "Epoch 5000, MSE: 0.5270358877580862\n",
            "Epoch 6000, MSE: 0.5168828761371292\n",
            "Epoch 7000, MSE: 0.5094293569626595\n",
            "Epoch 8000, MSE: 0.5036698397311112\n",
            "Epoch 9000, MSE: 0.49903787576269937\n",
            "Epoch 10000, MSE: 0.4951935006667101\n",
            "Waktu eksekusi: 109.00974559783936 detik\n",
            "Epochs: 10000, MSE: 0.4951935006667101\n",
            "Output: [2, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0, 1, 2, 0, 0, 2, 0, 0, 0, 2, 1, 0, 2, 0, 1, 1, 1, 0, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "True : [2, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0]\n",
            "Accuracy: 0.8333333333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X = minmax_scale(wine.data)\n",
        "Y = onehot_enc(wine.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
        "test_size=.3,random_state=1)\n",
        "#Isi jumlah layer yang digunakan dengan jumlah hidden layer 8\n",
        "w, ep, mse = bp_fit_tanh(X_train, y_train, layer_conf=(13, 8, 3), learn_rate=0.1, max_epoch=10000, max_error=0.01, print_per_epoch=1000)\n",
        "\n",
        "print(f'Epochs: {ep}, MSE: {mse}')\n",
        "\n",
        "predict = bp_predict_tanh(X_test, w)\n",
        "predict = onehot_dec(predict)\n",
        "y_test = onehot_dec(y_test)\n",
        "accuracy = accuracy_score(predict, y_test)\n",
        "\n",
        "print('Output:', predict)\n",
        "print('True :', y_test)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp6y7VWfjVEX"
      },
      "source": [
        "# Pertanyaan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP9dzq-kin0y"
      },
      "source": [
        "1.  Apa perbedaan dari penggunaan fungsi aktivasi sigmoid dengan fungsi aktivasi hyperbolic tangent?\n",
        "2. Coba jelaskan alasan dari perbedaan tersebut sebisa kalian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHEJApRcjXu7"
      },
      "source": [
        "# Jawaban"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S55HVfLjaZ5"
      },
      "source": [
        "1.  Fungsi aktivasi sigmoid dan hyperbolic tangent (tanh) memiliki perbedaan signifikan dalam rentang output, sifat simetris, dan respons terhadap input. Sigmoid menghasilkan output dalam rentang (0, 1) dan tidak simetris, yang dapat menyebabkan bias dalam representasi data, terutama jika input tidak terpusat di sekitar nol. Di sisi lain, tanh menghasilkan output dalam rentang (-1, 1) dan simetris, memberikan representasi yang lebih seimbang dan efektif untuk data yang berpusat di nol. Derivatif tanh juga lebih informatif dan tidak mengalami masalah vanishing gradient secepat sigmoid, membuat tanh lebih disukai dalam lapisan tersembunyi jaringan saraf. Sementara sigmoid sering digunakan di lapisan output untuk klasifikasi biner, tanh lebih umum di lapisan tersembunyi karena kemampuannya untuk mempercepat konvergensi selama pelatihan.\n",
        "\n",
        "2.  Perbedaan utama antara kedua fungsi ini terletak pada cara mereka merespons input dan seberapa baik mereka dapat mempertahankan gradien saat dilatih. Fungsi tanh mengatasi beberapa kelemahan fungsi sigmoid dengan memberikan rentang output yang lebih baik, derivate yang lebih informatif, dan simetri yang lebih menguntungkan. Ini membuat tanh menjadi pilihan yang lebih baik dalam banyak situasi, terutama untuk lapisan tersembunyi dalam jaringan saraf yang dalam."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}