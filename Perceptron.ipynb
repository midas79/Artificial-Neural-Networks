{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT5JA68cGRD-"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM_f45c-Ijrt"
      },
      "source": [
        "## Fungsi-fungsi *Plotting*\n",
        "\n",
        "Jangan mengubah kode pada *cell* di bawah ini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o61nxTlWImIj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def line(w, th=0):\n",
        "    w2 = w[2] + .001 if w[2] == 0 else w[2]\n",
        "\n",
        "    return lambda x: (th - w[1] * x - w[0]) / w2\n",
        "\n",
        "\n",
        "def plot(f1, f2, X, target, padding=1, marker='o'):\n",
        "    X = np.array(X)\n",
        "\n",
        "    x_vals, y_vals = X[:, 1], X[:, 2]\n",
        "    xmin, xmax, ymin, ymax = x_vals.min(), x_vals.max(), y_vals.min(), y_vals.max()\n",
        "    markers = f'r{marker}', f'b{marker}'\n",
        "    line_x = np.arange(xmin-padding-1, xmax+padding+1)\n",
        "\n",
        "    for c, v in enumerate(np.unique(target)):\n",
        "        p = X[np.where(target == v)]\n",
        "\n",
        "        plt.plot(p[:,1], p[:,2], markers[c])\n",
        "\n",
        "    plt.axis([xmin-padding, xmax+padding, ymin-padding, ymax+padding])\n",
        "    plt.plot(line_x, f1(line_x))\n",
        "    plt.plot(line_x, f2(line_x))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeQ-hZXRIzQ8"
      },
      "source": [
        "## Praktikum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGprbYbtGT4Y"
      },
      "source": [
        "### a) Fungsi *Step* Perceptron\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6OyljkSD9gF"
      },
      "outputs": [],
      "source": [
        "def percep_step(input, th=0):\n",
        "    return 1 if input > th else -1 if input < -th else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9hqEaqjJBq_"
      },
      "source": [
        "### b) Fungsi *training* Perceptron\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-0KDVKBJFXE"
      },
      "outputs": [],
      "source": [
        "def percep_fit(X, target, th=0, a=1, max_epoch=-1, verbose=False,\n",
        "draw=False):\n",
        "    w = np.zeros(len(X[0]) + 1)\n",
        "    bias = np.ones((len(X), 1))\n",
        "    X = np.hstack((bias, X))\n",
        "    stop = False\n",
        "    epoch = 0\n",
        "\n",
        "    while not stop and (max_epoch == -1 or epoch < max_epoch):\n",
        "        stop = True\n",
        "        epoch += 1\n",
        "\n",
        "        if verbose:\n",
        "            print('\\nEpoch', epoch)\n",
        "\n",
        "        for r, row in enumerate(X):\n",
        "            y_in = np.dot(row, w)\n",
        "            y = percep_step(y_in, th)\n",
        "\n",
        "            if y != target[r]:\n",
        "                stop = False\n",
        "\n",
        "            w = [w[i] + a * target[r] * row[i] for i in range(len(row))]\n",
        "\n",
        "            if verbose:\n",
        "                print('Bobot:', w)\n",
        "\n",
        "            if draw:\n",
        "                plot(line(w, th), line(w, -th), X, target)\n",
        "\n",
        "    return w, epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdO8Nce6JHpv"
      },
      "source": [
        "### c) Fungsi *testing* Perceptron\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "081BDbRiJH-p"
      },
      "outputs": [],
      "source": [
        "def percep_predict(X, w, th=0):\n",
        "    Y = []\n",
        "\n",
        "    for x in X:\n",
        "        y_in = w[0] + np.dot(x, w[1:])\n",
        "        y = percep_step(y_in, th)\n",
        "\n",
        "        Y.append(y)\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkiVVThWvvIU"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(a, b):\n",
        "    s = [1 if a[i] == b[i] else 0 for i in range(len(a))]\n",
        "\n",
        "    return sum(s) / len(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N582PIgcJZ_X"
      },
      "source": [
        "### d) Logika AND\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sObj6GCmJemo"
      },
      "outputs": [],
      "source": [
        "train = (1, 1), (1, -1), (-1, 1), (-1, -1)\n",
        "target = 1, -1, -1, -1\n",
        "th = .2\n",
        "model, epoch = percep_fit(train, target, th, verbose=True,\n",
        "draw=True)\n",
        "output = percep_predict(train, model)\n",
        "accuracy = calc_accuracy(output, target)\n",
        "\n",
        "print('Epochs:', epoch)\n",
        "print('Output:', output)\n",
        "print('Target:', target)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZpzmx1dJfAm"
      },
      "source": [
        "### e) Logika OR\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxgO2LChJjLI"
      },
      "outputs": [],
      "source": [
        "train = (1, 1), (1, -1), (-1, 1), (-1, -1)\n",
        "target = 1, 1, 1, -1\n",
        "th = .2\n",
        "model, epoch = percep_fit(train, target, th, verbose=True,\n",
        "draw=True)\n",
        "output = percep_predict(train, model)\n",
        "accuracy = calc_accuracy(output, target)\n",
        "\n",
        "print('Epochs:', epoch)\n",
        "print('Output:', output)\n",
        "print('Target:', target)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtHALXAdJjnE"
      },
      "source": [
        "### f) Logika AND NOT\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJMSZ7tJJnNS"
      },
      "outputs": [],
      "source": [
        "train = (1, 1), (1, -1), (-1, 1), (-1, -1)\n",
        "target = -1, 1, -1, -1\n",
        "th = .2\n",
        "model, epoch = percep_fit(train, target, th, verbose=True,\n",
        "draw=True)\n",
        "output = percep_predict(train, model)\n",
        "accuracy = calc_accuracy(output, target)\n",
        "\n",
        "print('Epochs:', epoch)\n",
        "print('Output:', output)\n",
        "print('Target:', target)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywkGWesWJnpe"
      },
      "source": [
        "### g) Logika XOR\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96ziWHGgJoCm"
      },
      "outputs": [],
      "source": [
        "train = (1, 1), (1, -1), (-1, 1), (-1, -1)\n",
        "target = -1, 1, 1, -1\n",
        "th = .2\n",
        "model, epoch = percep_fit(train, target, th, max_epoch=50,\n",
        "verbose=True, draw=False)\n",
        "output = percep_predict(train, model)\n",
        "accuracy = calc_accuracy(output, target)\n",
        "\n",
        "print('Output:', output)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9mk4MuynpzW"
      },
      "source": [
        "## *Dataset* Iris\n",
        "\n",
        "![Iris Dataset](https://www.spataru.at/images/blog/iris-dataset-svm/iris_types.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8z_b6JGtxct"
      },
      "source": [
        "### h) *Load* dan *plot* data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rkk7y2CMtyFX"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = sns.load_dataset('iris')\n",
        "\n",
        "sns.pairplot(iris, hue='species')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jus1YKJTt29L"
      },
      "source": [
        "### i) Menghapus Kelas Virginica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnYqOKvat3XD"
      },
      "outputs": [],
      "source": [
        "iris = iris.loc[iris['species'] != 'virginica']\n",
        "\n",
        "sns.pairplot(iris, hue='species')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nCkBFZOt9uG"
      },
      "source": [
        "### j) Menghapus ciri `sepal_width` dan `petal_width`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz6bT0vut-y_"
      },
      "outputs": [],
      "source": [
        "iris = iris.drop(['sepal_width', 'petal_width'], axis=1)\n",
        "\n",
        "sns.pairplot(iris, hue='species')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCz6VDDguBLu"
      },
      "source": [
        "### k) Proses *Training* dan *Testing*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdpDyXI4uB1_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "X = iris[['sepal_length', 'petal_length']].to_numpy()\n",
        "X = minmax_scale(X)\n",
        "\n",
        "y = iris['species'].to_numpy()\n",
        "c = {'setosa': -1, 'versicolor': 1}\n",
        "y = [c[i] for i in y]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
        "w, epoch = percep_fit(X_train, y_train, verbose=True, draw=True)\n",
        "out = percep_predict(X_test, w)\n",
        "accuracy = calc_accuracy(out, y_test)\n",
        "\n",
        "print('Epochs:', epoch)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjLnI8RSzsvX"
      },
      "source": [
        "## Analisis\n",
        "\n",
        "1. Mengapa Perceptron gagal dalam melakukan proses training menggunakan data logika XOR? Jelaskan.\n",
        "\n",
        " - Perceptron gagal dalam melakukan proses training untuk data logika XOR karena keterbatasannya yang hanya dapat memisahkan data yang linearly separable. Perceptron bekerja dengan mencari garis lurus (dalam dua dimensi) atau hyperplane (dalam dimensi yang lebih tinggi) untuk memisahkan kelas-kelas data. Namun, data XOR memiliki pola di mana tidak ada satu garis lurus yang bisa memisahkan dua kelas dengan benar. Misalnya, titik (1,1)(1, 1)(1,1) dan (−1,−1)(-1, -1)(−1,−1) memiliki label -1, sementara (1,−1)(1, -1)(1,−1) dan (−1,1)(-1, 1)(−1,1) memiliki label 1. Keteraturan ini membuat perceptron gagal, karena memerlukan lebih dari satu garis untuk memisahkan titik-titik tersebut.\n",
        "\n",
        " - Untuk memecahkan masalah XOR, diperlukan model yang lebih kompleks yang mampu menangani data yang tidak dapat dipisahkan secara linear. Salah satu solusi yang lebih umum adalah menggunakan multi-layer perceptron (MLP) atau jaringan saraf tiruan yang memiliki lapisan tersembunyi dan fungsi aktivasi non-linear. Dengan adanya lapisan tersembunyi ini, model dapat mempelajari representasi yang lebih kompleks dan memisahkan data XOR dengan benar. Jadi, perceptron tunggal tidak cukup untuk kasus seperti ini, tetapi jaringan saraf yang lebih dalam dan kompleks bisa menyelesaikannya.\n",
        "\n",
        "2. Lakukan pelatihan data logika AND dengan learning rate yang berbeda-beda. Amati jumlah epoch yang dilakukan. Bagaimanakah efeknya pada proses pelatihan?\n",
        "\n",
        " - Dengan learning rate yang kecil seperti 0.01, model akan memerlukan lebih\n",
        " banyak epoch untuk mencapai konvergensi karena pembaruan bobot terjadi\n",
        " perlahan.\n",
        " - Learning rate yang sedang, seperti 0.1, biasanya memberikan hasil yang\n",
        " optimal, di mana pelatihan cepat konvergen dan jumlah epoch lebih sedikit.\n",
        " - Dengan learning rate yang besar seperti 0.5 atau 1.0, model mungkin\n",
        " memerlukan lebih sedikit epoch, tetapi ada risiko model tidak stabil atau\n",
        " tidak konvergen, karena langkah pembaruan bobot terlalu besar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Learning Rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = [(1, 1), (1, -1), (-1, 1), (-1, -1)]\n",
        "target = [1, -1, -1, -1]\n",
        "th = 0.2\n",
        "\n",
        "learning_rate = 0.01\n",
        "model, epoch = percep_fit(train, target, th, a=learning_rate, verbose=True, draw=False)\n",
        "output = percep_predict(train, model)\n",
        "accuracy = calc_accuracy(output, target)\n",
        "\n",
        "print(f'Learning Rate: {learning_rate}')\n",
        "print('Epochs:', epoch)\n",
        "print('Output:', output)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Learning Rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = [(1, 1), (1, -1), (-1, 1), (-1, -1)]\n",
        "target = [1, -1, -1, -1]\n",
        "th = 0.2\n",
        "\n",
        "learning_rate = 0.1\n",
        "model, epoch = percep_fit(train, target, th, a=learning_rate, verbose=True, draw=False)\n",
        "output = percep_predict(train, model)\n",
        "accuracy = calc_accuracy(output, target)\n",
        "\n",
        "print(f'Learning Rate: {learning_rate}')\n",
        "print('Epochs:', epoch)\n",
        "print('Output:', output)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Learning Rate = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = [(1, 1), (1, -1), (-1, 1), (-1, -1)]\n",
        "target = [1, -1, -1, -1]\n",
        "th = 0.2\n",
        "\n",
        "learning_rate = 0.5\n",
        "model, epoch = percep_fit(train, target, th, a=learning_rate, verbose=True, draw=False)\n",
        "output = percep_predict(train, model)\n",
        "accuracy = calc_accuracy(output, target)\n",
        "\n",
        "print(f'Learning Rate: {learning_rate}')\n",
        "print('Epochs:', epoch)\n",
        "print('Output:', output)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Learning Rate = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = [(1, 1), (1, -1), (-1, 1), (-1, -1)]\n",
        "target = [1, -1, -1, -1]\n",
        "th = 0.2\n",
        "\n",
        "learning_rate = 1.0\n",
        "model, epoch = percep_fit(train, target, th, a=learning_rate, verbose=True, draw=False)\n",
        "output = percep_predict(train, model)\n",
        "accuracy = calc_accuracy(output, target)\n",
        "\n",
        "print(f'Learning Rate: {learning_rate}')\n",
        "print('Epochs:', epoch)\n",
        "print('Output:', output)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTaqJAE90GcE"
      },
      "source": [
        "## Kesimpulan\n",
        "\n",
        "1. Jelaskan perbedaan antara Perceptron dengan Hebb net?\n",
        " - Perceptron adalah model jaringan saraf sederhana yang digunakan untuk klasifikasi biner. Perceptron menggunakan fungsi aktivasi step untuk memutuskan keluaran apakah 1 atau -1 (atau 0 dan 1). Dalam pelatihan, perceptron memperbarui bobot berdasarkan kesalahan antara prediksi dan target menggunakan aturan pembaruan perceptron. Perceptron mampu menyelesaikan masalah linear separable, tetapi tidak dapat menangani kasus non-linear separable seperti XOR.\n",
        " - Hebb Net, di sisi lain, mengikuti Hebbian Learning Rule yang didasarkan pada prinsip \"cells that fire together wire together.\" Dalam Hebb Net, bobot diperbarui jika ada korelasi positif antara input dan output. Hebb Net tidak menggunakan fungsi aktivasi seperti perceptron, dan tidak memperhitungkan kesalahan saat memperbarui bobot. Hebb Net juga lebih sederhana dan tidak memiliki mekanisme yang bisa mengatasi kasus yang lebih rumit.\n",
        "\n",
        "2. Mengapa learning rate dibutuhkan?\n",
        " - Learning rate adalah parameter penting dalam algoritma pembelajaran yang menentukan seberapa besar langkah yang diambil saat memperbarui bobot jaringan. Learning rate dibutuhkan untuk mengontrol kecepatan pembelajaran model. Jika learning rate terlalu kecil, pembaruan bobot akan lambat, dan model membutuhkan banyak epoch untuk mencapai konvergensi, yang berarti proses pelatihan menjadi sangat lambat. Sebaliknya, jika learning rate terlalu besar, model bisa melompat jauh melewati solusi optimal, menyebabkan ketidakstabilan dan kemungkinan model tidak konvergen. Dengan learning rate yang tepat, proses pembaruan bobot bisa lebih cepat dan stabil.\n",
        "\n",
        "3. Menurut Anda, kasus seperti apa yang bisa diselesaikan dan tidak bisa diselesaikan oleh Perceptron?\n",
        " - Kasus yang bisa diselesaikan oleh Perceptron adalah masalah linear separable, yaitu ketika data dapat dipisahkan oleh sebuah garis lurus (atau hyperplane dalam dimensi yang lebih tinggi). Contoh kasus yang dapat diselesaikan oleh perceptron adalah logika AND dan OR. Dalam masalah-masalah ini, titik data dari kelas yang berbeda dapat dipisahkan dengan jelas menggunakan satu garis lurus.\n",
        " - Kasus yang tidak bisa diselesaikan oleh Perceptron adalah masalah non-linear separable, di mana data tidak dapat dipisahkan dengan satu garis lurus. Contoh terkenal dari kasus ini adalah logika XOR, di mana tidak ada satu garis lurus yang dapat memisahkan kelas-kelas data secara sempurna. Untuk menangani kasus seperti ini, diperlukan model yang lebih kompleks, seperti multi-layer perceptron (MLP) yang menggunakan lapisan tersembunyi dan fungsi aktivasi non-linear untuk memisahkan data yang tidak dapat dipisahkan secara linear.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
